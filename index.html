<!DOCTYPE html>
<html>

<head>
    <title>My Data Science Portfolio</title>
</head>

<body>

    <!-- Introduction Section -->
    <section>
        <h2>Explore My Data Science Portfolio</h2>
        <p>Are you looking for a dedicated and skilled data scientist to join your team? Welcome to my Data Science Portfolio, where you can discover a diverse collection of projects that highlight my expertise and versatility in the field of data science.</p>

        <h3>Why Consider Me for Your Data Science Team?</h3>
        <p>As an aspiring data scientist, I have carefully curated this portfolio to showcase my proficiency in applying data science principles to solve complex problems across different industries. Here's what makes me a valuable addition to your data science team:</p>

        <ul>
            <li><strong>A Multifaceted Approach:</strong> My portfolio reflects a multifaceted approach to data science. I've undertaken projects spanning various industries, from finance and healthcare to e-commerce and marketing. Each project demonstrates my ability to adapt data science techniques to meet industry-specific challenges.</li>
            <li><strong>Hands-On Expertise:</strong> I believe in learning by doing. Throughout my projects, I've rolled up my sleeves to collect, clean, and analyze data, ultimately deriving actionable insights. My hands-on experience sets me apart as a candidate who can turn data into real-world solutions.</li>
            <li><strong>A Commitment to Learning:</strong> The world of data science is dynamic and ever-evolving. I am dedicated to continuous learning and staying updated with the latest tools and technologies. My portfolio is a testament to my growth as a data scientist.</li>
            <li><strong>Problem-Solving at the Core:</strong> Data science is about problem-solving, and I thrive on tackling complex issues. Each project in my portfolio demonstrates my analytical mindset and my ability to unravel data puzzles to derive meaningful conclusions.</li>
            <li><strong>Collaboration and Knowledge Sharing:</strong> I believe in the power of collaboration and knowledge sharing. My participation in open-source projects and contributions to the data science community reflect my commitment to teamwork and fostering a culture of learning.</li>
            <li><strong>Adaptability and Innovation:</strong> In today's data-driven world, adaptability is crucial. My projects highlight my ability to innovate and adapt data science methodologies to address novel challenges, staying ahead of the curve.</li>
        </ul>

        <h3>Ready to Elevate Your Data Team?</h3>
        <p>My Data Science Portfolio is a testament to my dedication and passion for data science. Whether you're seeking a data scientist who can contribute to your organization's success or a collaborator for your data-driven initiatives, I am prepared to add value to your team.</p>
        <p>Explore my portfolio and connect with me to discuss how we can leverage data science to address your organization's unique needs. Together, we can harness the power of data to drive innovation and make informed decisions.</p>
    </section>

    <!-- Project Links Section -->
    <section>
        <h2>Explore My Projects</h2>
        <ul>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/Customer%20Segmentation">Customer Segmentation</a>
                <!-- Add the project summary here -->
                <!-- Project: Customer Segmentation with K-Means Clustering -->
<section>
    <h3>Customer Segmentation with K-Means Clustering</h3>
    <p>This project focuses on customer segmentation using K-Means clustering, a powerful unsupervised machine learning technique. The objective is to categorize customers into distinct groups based on their purchasing behavior using a dataset of sales transactions.</p>

    <h4>Key Steps:</h4>
    <ol>
        <li><strong>Data Loading:</strong> The project begins by loading a dataset containing sales transaction information. It includes features like transaction time, normalized data, and raw data.</li>
        <li><strong>Data Preprocessing and Scaling:</strong> The data is standardized using StandardScaler to ensure that all features have the same scale. This step is crucial for K-Means clustering.</li>
        <li><strong>Determining the Optimal Number of Clusters:</strong> The Elbow Method and Silhouette Score are used to find the optimal number of clusters. This helps decide the appropriate division of customers.</li>
        <li><strong>K-Means Clustering:</strong> K-Means clustering is performed with the optimal number of clusters. Customer data is grouped into clusters, and cluster labels are added to the original dataset.</li>
        <li><strong>Visualization:</strong> The clusters are visualized using the first two principal components, providing insights into the grouping of customers based on their purchase behavior.</li>
        <li><strong>Interpretation of Clusters:</strong> The project interprets the clusters by calculating the mean values of features within each cluster. This analysis helps understand the characteristics of customers in each group.</li>
        <li><strong>Conclusion and Recommendations:</strong> Based on the cluster characteristics, the project provides recommendations for marketing and business strategies tailored to each customer segment. These recommendations aim to enhance customer engagement and satisfaction.</li>
    </ol>
</section>

                <!-- End of project summary -->
            </li>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/Fraud%20Detection">Fraud Detection</a>
                <!-- Add the project summary here -->
                <!-- Project: Fraud Detection with Random Forest Classifier -->
<section>
    <h3>Fraud Detection with Random Forest Classifier</h3>
    <p>This project focuses on fraud detection using a Random Forest Classifier, a popular machine learning algorithm for classification tasks. The objective is to identify fraudulent transactions in a dataset and evaluate the model's performance.</p>

    <h4>Key Steps:</h4>
    <ol>
        <li><strong>Data Loading:</strong> The project begins by loading the dataset from a CSV file. The dataset likely contains information about transactions, including features like transaction time, card type, location, purchase category, transaction amount, customer age, and more.</li>
        <li><strong>Data Preprocessing:</strong> The data is preprocessed to prepare it for machine learning. The 'transaction_time' column is converted to datetime, and additional features like 'hour' and 'day_of_week' are extracted, which can provide valuable temporal information.</li>
        <li><strong>Feature Engineering:</strong> The project selects columns for one-hot encoding and standardization. Categorical features such as 'card_type', 'location', and 'purchase_category' are one-hot encoded, while numerical features like 'amount', 'customer_age', 'hour', and 'day_of_week' are standardized.</li>
        <li><strong>Column Transformation:</strong> The 'ColumnTransformer' is used to apply the defined transformations to the dataset efficiently.</li>
        <li><strong>Data Splitting:</strong> The dataset is split into training and testing sets using the 'train_test_split' function. This step ensures that the model's performance can be evaluated on unseen data.</li>
        <li><strong>Model Training:</strong> A Random Forest Classifier is initialized and trained using the training data. The 'n_estimators' parameter is set to 100, and 'random_state' is fixed for reproducibility.</li>
        <li><strong>Model Evaluation:</strong> The trained model is used to make predictions on the testing set. Confusion matrix and classification report metrics are calculated to assess the model's performance in detecting fraudulent transactions.</li>
        <li><strong>Results Output:</strong> The project concludes by printing the results, including the confusion matrix and classification report, which provide insights into the model's accuracy, precision, recall, and F1-score in identifying fraudulent transactions.</li>
    </ol>
</section>

                <!-- Your project summary for "Fraud Detection" here -->
            </li>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/Movie%20Recommendation">Movie Recommendation</a>
                <!-- Add the project summary here -->
                <!-- Project: Movie Recommendation -->
<section>
    <h3>Movie Recommendation</h3>
    <p>The Movie Recommendation project focuses on building a movie recommendation system that suggests movies to users based on their preferences and movie similarities. This system combines both content-based and collaborative filtering approaches to provide personalized movie recommendations.</p>

    <h4>Key Steps:</h4>
    <ol>
        <li><strong>Data Loading:</strong> The project begins by loading two essential datasets, 'movies.csv' and 'ratings.csv,' which contain information about movies and user ratings.</li>
        <li><strong>Content-Based Filtering:</strong> Content-based filtering involves analyzing movie genres using TF-IDF (Term Frequency-Inverse Document Frequency) and computing cosine similarities between movies based on their genre descriptions. This helps identify movies with similar content.</li>
        <li><strong>Collaborative Filtering:</strong> Collaborative filtering is implemented by creating a user-item matrix of movie ratings. Singular Value Decomposition (SVD) is applied to this matrix to extract latent factors, and predicted movie ratings are generated. Nearest Neighbors (KNN) is used to find similar users based on their movie preferences, and recommendations are made based on their preferences.</li>
        <li><strong>Hybrid Recommendations:</strong> The project offers hybrid recommendations, combining content-based and collaborative filtering. Users can receive movie suggestions based on their preferences and movie similarities.</li>
    </ol>
    <p>The movie recommendation system provides users with personalized movie suggestions, enhancing their movie-watching experience by recommending movies that align with their interests and preferences.</p>
</section>

                <!-- Your project summary for "Movie Recommendation" here -->
            </li>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/NBA%20Stat%20Project">NBA Stat Project</a>
                <!-- Add the project summary here -->
                <!-- Project: NBA Stat Project -->
<section>
    <h3>NBA Stat Project</h3>
    <p>The NBA Stat Project is a data science project focused on analyzing and modeling NBA player statistics to gain insights into player performance, cluster players based on their performance, and predict player points. This project involves data preprocessing, feature engineering, exploratory data analysis (EDA), clustering analysis, and predictive modeling.</p>

    <h4>Key Steps:</h4>
    <ol>
        <li><strong>Data Loading:</strong> The project begins by loading NBA player statistics data from a CSV file, and it forms the foundation for all subsequent analysis.</li>
        <li><strong>Preprocessing:</strong> Data preprocessing includes converting the 'min' column to minutes, handling missing values by imputing with mean values, and other data cleaning tasks.</li>
        <li><strong>Feature Engineering:</strong> A new feature 'PER' (Player Efficiency Rating) is calculated based on various player statistics to quantify player performance.</li>
        <li><strong>EDA and Visualization:</strong> Exploratory Data Analysis is performed to understand the relationships between different player statistics. A correlation matrix is visualized to identify key correlations.</li>
        <li><strong>Player Clustering:</strong> Players are clustered into groups using K-Means clustering based on selected performance features. PCA is applied for visualization.</li>
        <li><strong>Predictive Modeling:</strong> A Linear Regression model is built to predict player points based on relevant statistics. The model's performance is evaluated using Mean Squared Error (MSE).</li>
    </ol>
    <p>This project provides valuable insights into NBA player performance, helps identify player clusters with similar attributes, and offers a predictive model for estimating player points based on their statistics.</p>
</section>

                <!-- Your project summary for "NBA Stat Project" here -->
            </li>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/Product%20Bundling%20Project">Product Bundling Project</a>
                <!-- Add the project summary here -->
                <!-- Project: Product Bundling Recommendation System -->
<section>
    <h3>Product Bundling Recommendation System</h3>
    <h4>Objective</h4>
    <p>The objective of this project is to build a sophisticated and efficient product bundling recommendation system. By leveraging data science techniques, we aim to analyze buying habits and suggest bundles of products that are often bought together by customers.</p>

    <h4>Steps</h4>
    <ol>
        <li><strong>Data Preprocessing:</strong> We begin by loading the dataset and performing data preprocessing tasks, including handling missing values, data type conversions, and scaling.</li>
        <li><strong>Exploratory Data Analysis (EDA):</strong> EDA is a critical step to understand the data's structure, distribution, and patterns. We explore summary statistics, distributions, and correlations among features.</li>
        <li><strong>Model Building:</strong> For product bundling, we employ two main techniques: K-Means Clustering to segment customers based on buying habits and Association Rule Mining to discover associations between different products.</li>
        <li><strong>Evaluation:</strong> We evaluate the model's performance and generate association rules to find product associations that can be valuable for recommendations.</li>
        <li><strong>Conclusion:</strong> In the conclusion, we summarize the project's outcomes and highlight the potential benefits for businesses seeking to increase sales through effective product bundling strategies.</li>
    </ol>
    <p>By implementing this recommendation system, businesses can optimize their product offerings and enhance customer satisfaction.</p>
</section>

                <!-- Your project summary for "Product Bundling Project" here -->
            </li>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/Real%20Estate%20Time%20Series%20Analysis">Real Estate Time Series Analysis</a>
                <!-- Add the project summary here -->
                <!-- Project: Real Estate Time Series Analysis -->
<section>
    <h3>Real Estate Time Series Analysis</h3>
    <p>The Real Estate Time Series Analysis project focuses on analyzing time series data related to the real estate market. It aims to provide insights and predictions based on historical real estate data, which can be valuable for investors and market analysis.</p>

    <h4>Key Steps:</h4>
    <ol>
        <li><strong>Data Collection:</strong> The project starts by collecting historical real estate market data, including property prices, location, time, and other relevant features.</li>
        <li><strong>Data Preprocessing:</strong> Data preprocessing tasks involve cleaning the data, handling missing values, and ensuring data is in a suitable format for analysis.</li>
        <li><strong>Exploratory Data Analysis (EDA):</strong> EDA is conducted to understand trends, patterns, and correlations within the real estate data. Visualizations and summary statistics are used to gain insights.</li>
        <li><strong>Time Series Analysis:</strong> Time series modeling techniques, such as ARIMA or Prophet, are applied to make predictions about future real estate market trends and property prices.</li>
        <li><strong>Visualization and Reporting:</strong> The project concludes by presenting visualizations and insights derived from the time series analysis, offering valuable information for real estate investors and stakeholders.</li>
    </ol>
    <p>This project empowers real estate professionals and investors with data-driven insights to make informed decisions in the dynamic real estate market.</p>
</section>
                <!-- Your project summary for "Real Estate Time Series Analysis" here -->
            </li>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/SQLYouTube">SQL YouTube</a>
                <!-- Add the project summary here -->
                <!-- Project: SQL Video Statistics Analysis -->
<section>
    <h3>SQL Video Statistics Analysis</h3>
    <p>The SQL Video Statistics Analysis project involves querying and analyzing video statistics data using SQL queries. It focuses on extracting valuable insights from the data, such as like-to-dislike ratios, average comments per video by category, popular days for comments, top words in comments, and more.</p>

    <h4>Key SQL Queries:</h4>
    <ol>
        <li><strong>Query 1: Highest and Lowest Like-to-Dislike Ratios</strong><br>
            Find the videos with the highest and lowest like-to-dislike ratios in the dataset.
        </li>
        <li><strong>Query 2: Average Comments per Video by Category</strong><br>
            Calculate the average number of comments per video for each category.
        </li>
        <li><strong>Query 3: Most Commented Day of the Week</strong><br>
            Determine which day of the week receives the most comments.
        </li>
        <li><strong>Query 4: Top 3 Most Frequent Words in Comments</strong><br>
            Identify the top 3 most frequent words used in comments.
        </li>
        <li><strong>Query 5: Videos with More Likes Than Average</strong><br>
            Find out how many videos have more likes than the average number of likes across all videos.
        </li>
        <li><strong>Query 6: Percentage of Total Views by Category</strong><br>
            Calculate the percentage of total views that each category represents.
        </li>
        <li><strong>Query 7: Videos in the Top 5 for Likes and Comments</strong><br>
            Identify videos that appear in the top 5 for both most likes and most comments.
        </li>
        <li><strong>Query 8: Average Views for Videos with Above-Average Likes</strong><br>
            Calculate the average number of views for videos that have above-average likes.
        </li>
        <li><strong>Query 9: Videos with Sentiment Scores Higher Than Average</strong><br>
            Find videos with sentiment scores higher than the average sentiment score.
        </li>
        <li><strong>Query 10: Correlation Between Views and Comments</strong><br>
            Calculate the correlation between views and comments in the dataset.
        </li>
    </ol>
</section>

                <!-- Your project summary for "SQL YouTube" here -->
            </li>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/Stock%20Predection">Stock Prediction</a>
                <!-- Add the project summary here -->
                <!-- Your project summary for "Stock Prediction" here -->
            </li>
            <li>
                <a href="https://github.com/JustinBustamante/Data_Science-Journey/tree/main/YouTube%20Sentiment%20Analysis">YouTube Sentiment Analysis</a>
                <!-- Add the project summary here -->
                <!-- Your project summary for "YouTube Sentiment Analysis" here -->
            </li>
        </ul>
    </section>

    <!-- Add more project links as needed -->
</body>

</html>
